{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import ClassVar, Optional, TypeVar, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from torch.utils.data import DataLoader, Dataset, default_collate\n",
    "\n",
    "from opendataval.dataloader.util import CatDataset\n",
    "\n",
    "Self = TypeVar(\"Self\", bound=\"Model\")\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"Abstract class of Models. Provides a template for models.\"\"\"\n",
    "\n",
    "    Models: ClassVar[dict[str, Self]] = {}\n",
    "\n",
    "    def __init_subclass__(cls, *args, **kwargs):\n",
    "        \"\"\"Registers Model types, used as part of the CLI.\"\"\"\n",
    "        super().__init_subclass__(*args, **kwargs)\n",
    "        cls.Models[cls.__name__.lower()] = cls\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        *args,\n",
    "        sample_weights: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Self:\n",
    "        \"\"\"Fits the model on the training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor | Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        args : tuple[Any]\n",
    "            Additional positional args\n",
    "        sample_weights : torch.Tensor, optional\n",
    "            Weights associated with each data point, must be passed in as key word arg,\n",
    "            by default None\n",
    "        kwargs : dict[str, Any]\n",
    "            Addition key word args\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self for api consistency with sklearn.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x: Union[torch.Tensor, Dataset], *args, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"Predict the label from the input covariates data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor | Dataset\n",
    "            Input data covariates\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output predictions based on the input\n",
    "        \"\"\"\n",
    "\n",
    "    def clone(self) -> Self:\n",
    "        \"\"\"Clone Model object.\n",
    "\n",
    "        Copy and returns object representing current state. We often take a base\n",
    "        model and train it several times, so we need to have the same initial conditions\n",
    "        Default clone implementation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns deep copy of model.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "\n",
    "class TorchModel(Model, nn.Module):\n",
    "    \"\"\"Torch Models have a device they belong to and shared behavior\"\"\"\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "\n",
    "class TorchClassMixin(TorchModel):\n",
    "    \"\"\"Classifier Mixin for Torch Neural Networks.\"\"\"\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        batch_size: int = 32,\n",
    "        epochs: int = 1,\n",
    "        lr: float = 0.01,\n",
    "    ):\n",
    "        \"\"\"Fits the model on the training data.\n",
    "\n",
    "        Fits a torch classifier Model object using ADAM optimizer and cross\n",
    "        categorical entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor | Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        batch_size : int, optional\n",
    "            Training batch size, by default 32\n",
    "        epochs : int, optional\n",
    "            Number of training epochs, by default 1\n",
    "        sample_weights : torch.Tensor, optional\n",
    "            Weights associated with each data point, by default None\n",
    "        lr : float, optional\n",
    "            Learning rate for the Model, by default 0.01\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        criterion = F.binary_cross_entropy if self.num_classes == 2 else F.cross_entropy\n",
    "        dataset = CatDataset(x_train, y_train, sample_weight)\n",
    "\n",
    "        self.train()\n",
    "        for _ in range(int(epochs)):\n",
    "            # *weights helps check if we passed weights into the Dataloader\n",
    "            for x_batch, y_batch, *weights in DataLoader(\n",
    "                dataset, batch_size, shuffle=True, pin_memory=True\n",
    "            ):\n",
    "                # Moves data to correct device\n",
    "                x_batch = x_batch.to(device=self.device)\n",
    "                y_batch = y_batch.to(device=self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.__call__(x_batch)\n",
    "\n",
    "                if sample_weight is not None:\n",
    "                    # F.cross_entropy doesn't support sample_weights\n",
    "                    loss = criterion(outputs, y_batch, reduction=\"none\")\n",
    "                    loss = (loss * weights[0].to(device=self.device)).mean()\n",
    "                else:\n",
    "                    loss = criterion(outputs, y_batch, reduction=\"mean\")\n",
    "\n",
    "                loss.backward()  # Compute gradient\n",
    "                optimizer.step()  # Updates weights\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class TorchRegressMixin(TorchModel):\n",
    "    \"\"\"Regressor Mixin for Torch Neural Networks.\"\"\"\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        batch_size: int = 32,\n",
    "        epochs: int = 1,\n",
    "        lr: float = 0.01,\n",
    "    ):\n",
    "        \"\"\"Fits the regression model on the training data.\n",
    "\n",
    "        Fits a torch regression Model object using ADAM optimizer and MSE loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor | Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        batch_size : int, optional\n",
    "            Training batch size, by default 32\n",
    "        epochs : int, optional\n",
    "            Number of training epochs, by default 1\n",
    "        sample_weight : torch.Tensor, optional\n",
    "            Weights associated with each data point, by default None\n",
    "        lr : float, optional\n",
    "            Learning rate for the Model, by default 0.01\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        criterion = F.mse_loss\n",
    "        dataset = CatDataset(x_train, y_train, sample_weight)\n",
    "\n",
    "        self.train()\n",
    "        for _ in range(int(epochs)):\n",
    "            # *weights helps check if we passed weights into the Dataloader\n",
    "            for x_batch, y_batch, *weights in DataLoader(\n",
    "                dataset,\n",
    "                batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "            ):\n",
    "                # Moves data to correct device\n",
    "                x_batch = x_batch.to(device=self.device)\n",
    "                y_batch = y_batch.to(device=self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_hat = self.__call__(x_batch)\n",
    "\n",
    "                if sample_weight is not None:\n",
    "                    # F.cross_entropy doesn't support sample_weight\n",
    "                    loss = criterion(y_hat, y_batch, reduction=\"none\")\n",
    "                    loss = (loss * weights[0].to(device=self.device)).mean()\n",
    "                else:\n",
    "                    loss = criterion(y_hat, y_batch, reduction=\"mean\")\n",
    "\n",
    "                loss.backward()  # Compute gradient\n",
    "                optimizer.step()  # Updates weights\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class TorchPredictMixin(TorchModel):\n",
    "    \"\"\"Torch ``.predict()`` method mixin for Torch Neural Networks.\"\"\"\n",
    "\n",
    "    def predict(self, x: Union[torch.Tensor, Dataset]) -> torch.Tensor:\n",
    "        \"\"\"Predict output from input tensor/data set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input covariates\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Predicted tensor output\n",
    "        \"\"\"\n",
    "        if isinstance(x, Dataset):\n",
    "            x = next(iter(DataLoader(x, batch_size=len(x), pin_memory=True)))\n",
    "        x = x.to(device=self.device)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.__call__(x)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "def to_numpy(tensors: tuple[torch.Tensor]) -> tuple[torch.Tensor]:\n",
    "    \"\"\"Mini function to move tensor to CPU for sk-learn.\"\"\"\n",
    "    return tuple(t.numpy(force=True) for t in default_collate(tensors))\n",
    "\n",
    "\n",
    "class ClassifierSkLearnWrapper(Model):\n",
    "    \"\"\"Wrapper for sk-learn classifiers that can have weighted fit methods.\n",
    "\n",
    "    Example:\n",
    "    ::\n",
    "        wrapped = ClassifierSkLearnWrapper(LinearRegression(), 2)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : BaseModel\n",
    "        Any sk-learn model that supports ``sample_weights``\n",
    "    num_classes : int\n",
    "        Label dimensionality\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, num_classes: int, *args, **kwargs):\n",
    "        self.model = base_model(*args, **kwargs)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        *args,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Fits the model on the training data.\n",
    "\n",
    "        Fits a sk-learn wrapped classifier Model. If there are less classes in the\n",
    "        sample than num_classes, uses dummy model.\n",
    "        ::\n",
    "            wrapped = ClassifierSkLearnWrapper(MLPClassifier, 2)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor | Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        args : tuple[Any]\n",
    "            Additional positional args\n",
    "        sample_weights : torch.Tensor, optional\n",
    "            Weights associated with each data point, must be passed in as key word arg,\n",
    "            by default None\n",
    "        kwargs : dict[str, Any]\n",
    "            Addition key word args\n",
    "        \"\"\"\n",
    "        # Using a data set and dataloader (despite loading all the data) consistency\n",
    "        dataset = CatDataset(x_train, y_train, sample_weight)\n",
    "        num_samples = len(dataset)\n",
    "\n",
    "        if num_samples == 0:\n",
    "            self.model = DummyClassifier(strategy=\"constant\", constant=0).fit([0], [0])\n",
    "            self.model.n_classes_ = self.num_classes\n",
    "            return self\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=num_samples, collate_fn=to_numpy)\n",
    "        # *weights helps check if we passed weights into the Dataloader\n",
    "        x_train, y_train, *weights = next(iter(dataloader))\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        y_train_unique = np.unique(y_train)\n",
    "\n",
    "        with warnings.catch_warnings():  # Ignores warnings in the following block\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            if len(y_train_unique) != self.num_classes:  # All labels must be in sample\n",
    "                dummy_strat = \"most_frequent\"\n",
    "                self.model = DummyClassifier(strategy=dummy_strat).fit(x_train, y_train)\n",
    "                self.model.n_classes_ = self.num_classes\n",
    "            elif sample_weight is not None:\n",
    "                weights = np.squeeze(weights[0])\n",
    "                self.model.fit(x_train, y_train, *args, sample_weight=weights, **kwargs)\n",
    "            else:\n",
    "                self.model.fit(x_train, y_train, *args, sample_weight=None, **kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: Union[torch.Tensor, Dataset]) -> torch.Tensor:\n",
    "        \"\"\"Predict labels from sk-learn model.\n",
    "\n",
    "        Makes a prediction based on the input tensor. Uses the `.predict_proba(x)`\n",
    "        method on sk-learn classifiers. Output dim will match the input to\n",
    "        the `.train(x, y)` method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor | Dataset\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Extracts the input into a cpu tensor\n",
    "        if isinstance(x, Dataset):\n",
    "            x = next(iter(DataLoader(x, len(x)))).numpy(force=True)\n",
    "        else:\n",
    "            x = x.numpy(force=True)\n",
    "        output = self.model.predict_proba(x)\n",
    "\n",
    "        return torch.from_numpy(output).to(dtype=torch.float)\n",
    "\n",
    "\n",
    "class ClassifierUnweightedSkLearnWrapper(ClassifierSkLearnWrapper):\n",
    "    \"\"\"Wrapper for sk-learn classifiers that can don't have weighted fit methods.\n",
    "\n",
    "    Example:\n",
    "    ::\n",
    "        wrapped = ClassifierSkLearnWrapper(KNeighborsClassifier, 2)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : BaseModel\n",
    "        Any sk-learn model that supports ``sample_weights``\n",
    "    num_classes : int\n",
    "        Label dimensionality\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        *args,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Fits the model on the training data.\n",
    "\n",
    "        Fits a sk-learn wrapped classifier Model without sample weight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor | Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        args : tuple[Any]\n",
    "            Additional positional args\n",
    "        sample_weights : torch.Tensor, optional\n",
    "            Weights associated with each data point, must be passed in as key word arg,\n",
    "            by default None\n",
    "        kwargs : dict[str, Any]\n",
    "            Addition key word args\n",
    "        \"\"\"\n",
    "        # Using a data set and dataloader (despite loading all the data) for better\n",
    "        # API consistency, such as passing data sets to a sk-learn  model\n",
    "        dataset = CatDataset(x_train, y_train)\n",
    "        num_samples = len(dataset)\n",
    "        dataset = CatDataset(x_train, y_train, sample_weight)\n",
    "\n",
    "        if num_samples == 0:\n",
    "            self.model = DummyClassifier(strategy=\"constant\", constant=0).fit([0], [0])\n",
    "            self.model.n_classes_ = self.num_classes\n",
    "            return self\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=num_samples, collate_fn=to_numpy)\n",
    "        # *weights helps check if we passed weights into the Dataloader\n",
    "        x_train, y_train, *weights = next(iter(dataloader))\n",
    "        y_train = np.argmax(y_train, axis=1)\n",
    "        y_train_unique = np.unique(y_train)\n",
    "\n",
    "        with warnings.catch_warnings():  # Ignores warnings in the following block\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            if len(y_train_unique) != self.num_classes:  # All labels must be in sample\n",
    "                dummy_strat = \"most_frequent\"\n",
    "                self.model = DummyClassifier(strategy=dummy_strat).fit(x_train, y_train)\n",
    "                self.model.n_classes_ = self.num_classes\n",
    "            elif sample_weight is not None:\n",
    "                indices = np.random.choice(  # Random sample of the train data set\n",
    "                    num_samples,\n",
    "                    size=(num_samples),\n",
    "                    replace=True,\n",
    "                    p=weights[0].squeeze() / weights[0].sum(),\n",
    "                )\n",
    "                self.model.fit(x_train[indices], y_train[indices], *args, **kwargs)\n",
    "            else:\n",
    "                self.model.fit(x_train, y_train, *args, **kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "class RegressionSkLearnWrapper(Model):\n",
    "    \"\"\"Wrapper for sk-learn regression models.\n",
    "\n",
    "    Example:\n",
    "    ::\n",
    "        wrapped = RegressionSkLearnWrapper(LinearRegression)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model : BaseModel\n",
    "        Any sk-learn model that supports ``sample_weights``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model, *args, **kwargs):\n",
    "        self.model = base_model(*args, **kwargs)\n",
    "        self.num_classes = 1\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        x_train: Union[torch.Tensor, Dataset],\n",
    "        y_train: Union[torch.Tensor, Dataset],\n",
    "        *args,\n",
    "        sample_weight: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Fits the model on the training data.\n",
    "\n",
    "        Fits a sk-learn wrapped regression Model. If there is insufficient data to fit\n",
    "        a regression (such as len(x_train)==0), will use DummyRegressor that predicts\n",
    "        np.zeros((num_samples, self.num_classes))\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_train : torch.Tensor |  Dataset\n",
    "            Data covariates\n",
    "        y_train : torch.Tensor | Dataset\n",
    "            Data labels\n",
    "        args : tuple[Any]\n",
    "            Additional positional args\n",
    "        sample_weights : torch.Tensor, optional\n",
    "            Weights associated with each data point, must be passed in as key word arg,\n",
    "            by default None\n",
    "        kwargs : dict[str, Any]\n",
    "            Addition key word args\n",
    "        \"\"\"\n",
    "        # Using a data set and dataloader (despite loading all the data) for better\n",
    "        # API consistency, such as passing data sets to a sk-learn  model\n",
    "        dataset = CatDataset(x_train, y_train, sample_weight)\n",
    "        num_samples = len(dataset)\n",
    "\n",
    "        if num_samples == 0:\n",
    "            constant_return = np.zeros(shape=(1, self.num_classes))\n",
    "            self.model = DummyRegressor(strategy=\"mean\").fit([[0]], constant_return)\n",
    "            return self\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=num_samples, collate_fn=to_numpy)\n",
    "        # *weights helps check if we passed weights into the Dataloader\n",
    "        x_train, y_train, *weights = next(iter(dataloader))\n",
    "\n",
    "        with warnings.catch_warnings():  # Ignores warnings in the following block\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            if sample_weight is not None:\n",
    "                weights = np.squeeze(weights[0])\n",
    "                self.model.fit(x_train, y_train, *args, sample_weight=weights, **kwargs)\n",
    "            else:\n",
    "                self.model.fit(x_train, y_train, *args, sample_weight=None, **kwargs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: Union[torch.Tensor, Dataset]) -> torch.Tensor:\n",
    "        \"\"\"Predict values from sk-learn regression model.\n",
    "\n",
    "        Makes a prediction based on the input tensor. Uses the `.predict(x)`\n",
    "        method on sk-learn regression models. Output dim will match ``self.num_classes``\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor | Dataset\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # Extracts the input into a cpu tensor\n",
    "        if isinstance(x, Dataset):\n",
    "            x = next(iter(DataLoader(x, len(x)))).numpy(force=True)\n",
    "        else:\n",
    "            x = x.numpy(force=True)\n",
    "        output = self.model.predict(x).reshape(-1, self.num_classes)\n",
    "\n",
    "        return torch.from_numpy(output).to(dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
